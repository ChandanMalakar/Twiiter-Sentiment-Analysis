{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLsFVUV2bCSZ",
    "outputId": "eaf9f4ce-ae8f-4024-fd44-a796dc9ab5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "     ---------------------------------------- 0.0/82.7 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/82.7 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 51.2/82.7 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 82.7/82.7 kB 928.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.2)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105795 sha256=cfd18dd4568a103585d7dde5506e8117f8765756080859b689d28d86dd448466\n",
      "  Stored in directory: c:\\users\\chand\\appdata\\local\\pip\\cache\\wheels\\46\\d2\\26\\84d0a1acdb9c6baccf7d28cf06962ec80529fe1ad938489983\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script kaggle.exe is installed in 'C:\\Users\\chand\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8uxI1U0ihRlT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "visNOsD54S4j"
   },
   "outputs": [],
   "source": [
    "# API to fetch the dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-wUIVfc9Y38",
    "outputId": "5a8aaf32-d413-43b8-83fa-7783c8285f3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kaggle' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5OHwuGmQ_FvH"
   },
   "outputs": [],
   "source": [
    "# Extracting the compressed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRBXxQ8y_pJO",
    "outputId": "f15fefe2-d8e1-44c1-e84d-c9a983e3edf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find either sentiment140.zip or sentiment140.zip.zip.\n"
     ]
    }
   ],
   "source": [
    "!unzip sentiment140.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6sADKD9p_2az"
   },
   "outputs": [],
   "source": [
    "# Importing the dependencies for the Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dCvefx8OAFsh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # for regular expresssion - pattern matching\n",
    "from nltk.corpus import stopwords # for removing the un-influential words from the data, as they're of no use.\n",
    "from nltk.stem.porter import PorterStemmer # reducig the words to its root words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for converting the textual data into numerical form\n",
    "from sklearn.model_selection import train_test_split # for splitting the training data into 3 parts : train, test and split.\n",
    "from sklearn.linear_model import LogisticRegression # using the Logistic Regression ML model\n",
    "from sklearn.metrics import accuracy_score # for determining the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mic5At9Dyhy",
    "outputId": "c9009b90-6dde-4e9a-fe9f-c4aa457a59a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pj_5YqvECQv",
    "outputId": "29b559ec-2058-4bee-bf03-a8d4e047cb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Printing the stopwords in English\n",
    "print(stopwords.words('english'))\n",
    "# stopwords are those words in a sentence which doesn't carries any influential meaning to the data\n",
    "# hence these words will be removed from the data as they're not useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEW0ESjqFt1S"
   },
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rzhbQBkSFM_Z"
   },
   "outputs": [],
   "source": [
    "# loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L9yBmqa_FzU3"
   },
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('twitter_data.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybxh351JGOOW",
    "outputId": "2695695a-1793-4b5e-931b-8498b3153097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking its shape\n",
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "b-39cbGYJe7h",
    "outputId": "68178434-8a8d-4f61-c3ae-145940a1d849"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "zKKdTc9OXeIx"
   },
   "outputs": [],
   "source": [
    "# since there's no col names, manually writing the col names\n",
    "columns=['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "twitter_data = pd.read_csv('twitter_data.csv',names = columns, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xt__gjbaYUOi",
    "outputId": "b1052f2f-991e-4986-f1ea-e8e6980d417e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "IC2NE-W_YmHz",
    "outputId": "3d360e8e-f597-4632-a6f3-e68589001725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of missing values in the dataset\n",
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "e47XmY47Y34n",
    "outputId": "a4258acc-31de-4861-cb4a-05887313dbf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of the target column\n",
    "twitter_data['target'].value_counts()\n",
    "# here 0 -> means -ve\n",
    "# and 4 -> means +ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ReQDF5efZVFq"
   },
   "outputs": [],
   "source": [
    "# converting 4 to 1 for better understanding\n",
    "twitter_data.replace({'target':{4:1}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "1VcX1H3qZ2B6",
    "outputId": "7e204e63-a9d5-4007-ed7c-796ad525b05f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAeiVVcWepss"
   },
   "source": [
    "0 --> Negative Tweet\n",
    "\n",
    "1 --> Positive Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owlb_ILje7Qi"
   },
   "source": [
    "# **Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT6VI6X5fLFg"
   },
   "source": [
    "# Stemming is the process of reducing a words to its key word or root-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "x8rv5Ae9eu18"
   },
   "outputs": [],
   "source": [
    "# example : actor, actress, acting (words with similar meaning) = act (root word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Ifbq1QFrfcaa"
   },
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "z38Ov7gRfoOl"
   },
   "outputs": [],
   "source": [
    "def stemming(content): # here 'content' is the 'text' col from the dataset.\n",
    "\n",
    "  stemmed_content = re.sub('[^a-zA-Z]', ' ', content) # here we are removing all the characters which are not (a-z) & (A-Z)\n",
    "  # It is used to remove all the  punctuations and other symbols from the given text, which are not alphabets\n",
    "\n",
    "  stemmed_content = stemmed_content.lower() # converting to lowercase\n",
    "\n",
    "  stemmed_content = stemmed_content.split() # splitting the words from the processed text to form a list\n",
    "\n",
    "  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "  # here we are using the 'stopwords', means we are checking whether the 'stemmed_content' contains the stopwords or not\n",
    "  # If the 'stemmed_content' doesn't contains any stopwords,\n",
    "  # then the stemming function is applied over it to reduce the word to its root word\n",
    "\n",
    "  stemmed_content = ' '.join(stemmed_content)\n",
    "  # after the processing, we are again joining those words together\n",
    "\n",
    "  return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpGXAJCirmis"
   },
   "outputs": [],
   "source": [
    "twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f32_NuviXSw7"
   },
   "outputs": [],
   "source": [
    "temp_df = twitter_data['stemmed_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehKVHhfdr2Z0"
   },
   "outputs": [],
   "source": [
    " print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SaKAeEy-MYC"
   },
   "outputs": [],
   "source": [
    "print(twitter_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZExNnLMS-Q7S"
   },
   "outputs": [],
   "source": [
    "# Separating the data and label\n",
    "X = twitter_data['stemmed_content'].values\n",
    "Y = twitter_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GZg0pAF-gNE"
   },
   "outputs": [],
   "source": [
    " print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2bhoVUu-hxH"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Strsxfr8-tvu"
   },
   "source": [
    "# Splitting the data to training data & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TL6-ieX8-jVZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtt8vI1UANqh"
   },
   "source": [
    "## **test_size = 0.2**\n",
    "\n",
    "'test_size = 0.2' means 20% percent of data will go for test data and remaining 80% of data will go to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgXFcQeaASJj"
   },
   "source": [
    "## **stratify = Y**\n",
    "\n",
    "we are performing stratify on the data 'Y' because we need equal proportion of 2 classes (0 & 1) to go to both training & test data.\n",
    "\n",
    "we don't want it to be like all positive (1) go to training and all the negatives (0) to go to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzyrda9XAzZO"
   },
   "source": [
    "## **random_state = 2**\n",
    "\n",
    "It is used to separate the data in a random manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umXKyIYzA0t2"
   },
   "outputs": [],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vDluuD3Bfdz"
   },
   "outputs": [],
   "source": [
    "print(Y.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhPp9-NsCB3X"
   },
   "source": [
    "# Converting the textual data to numerical data\n",
    "\n",
    "converting the text to Numerical Values for ML model to understand ,\n",
    "here we are going to use a process called \"Feature Extraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpg2pz9YC9MC"
   },
   "source": [
    "## using the (vectorizer.fit_transform()) process each word in the training and test data will be given a weight/importance based on how many times it has been repeated, then the model will understand the sentiment of the word using this method.\n",
    "Example : if a word 'happy' is present 5 times and word 'fine' is present 2 times, then (vectorizer.fit_transform()) method will assign it 5 and 2 respectively. while training the model will use these numerical values (5,2) to understand that the word happy means more positive and fine means less positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECpakaQyCMPj"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train) # here 2 operations are being performed (a) fitting (b) transforming\n",
    "# (a) fitting is process of understanding the nature of the data\n",
    "# (b) transforming is conversion of the textual data to numerical values\n",
    "\n",
    "# Save the vectorizer\n",
    "with open(\"vectorizer.sav\", \"wb\") as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "X_test = vectorizer.transform(X_test)\n",
    "# here the test data is only transformed and not fitted since we already perfromed the fitting\n",
    "# here the test data is transformed based on the fitting of the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKKHodOIJun3"
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHifm1SYKpq7"
   },
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5Xz9VnNXw4R"
   },
   "source": [
    "# **Training the Machine Learning Model**\n",
    "\n",
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfHQvkKOXqDt"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8UhGRY1YQPQ"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "# X_train is the training data tweets\n",
    "# Y_train is the target for the training data tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJl7GyJjYyVn"
   },
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynxJpJFxZJcL"
   },
   "source": [
    "AccuracyScore : we are going to use AccuracyScore for Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl5NsZeKYuL_"
   },
   "outputs": [],
   "source": [
    "X_train_prediction = model.predict(X_train) # here the trained model predicts the output for X_train data\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_prediction) # we are now comparing the prediction with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUppd--OZIez"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy score on the training data :\", training_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQM0KMtAa7Ai"
   },
   "source": [
    "# Testing the accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mu-Tdttaqh_"
   },
   "outputs": [],
   "source": [
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(Y_test, X_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvQapOnPbNcF"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy score on the tes data :\", test_data_accuracy)\n",
    "# here our model has performed well as the accuracy of training data and of the test data is very close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYNJLgIbruT"
   },
   "source": [
    "## Overfitting occurs when the accuracy of the training data is very high while accuracy of test data is very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVTpfo7JcKvp"
   },
   "source": [
    "Model Accuracy = 77.6 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQhTnBH4cXzc"
   },
   "source": [
    "# Exporting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZdCJR0PbPOS"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI9I1O4XcZAz"
   },
   "outputs": [],
   "source": [
    "filename = 'trained_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gao8PQcBiLS5"
   },
   "source": [
    "# Using the Saved Model for future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq-DDrqjiJZq"
   },
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "loaded_model = pickle.load(open('/content/trained_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LnBM7gUiXDz"
   },
   "outputs": [],
   "source": [
    "X_new = X_test[200]\n",
    "print(Y_test[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGwPN8WYijL8"
   },
   "outputs": [],
   "source": [
    "prediction = loaded_model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0]==0):\n",
    "  print(\"Negative Tweet\")\n",
    "else:\n",
    "  print('Positive Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBYRTnzqitvO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
